{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff287a38-85a4-4648-b00a-608c0b4599ac",
   "metadata": {},
   "source": [
    "# Instalação e importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d09bcb-4fd2-4d2f-8c64-7acc6b59056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.11/site-packages (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.11/site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib scikit-learn opencv-python tensorflow keras opencv-python-headless keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97247da0-937f-4017-a8d0-07738429cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 23:39:07.240475: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-14 23:39:07.504026: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-14 23:39:07.787098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 23:39:08.010054: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 23:39:08.065253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 23:39:08.406445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 23:39:12.651906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec74dc-56d5-429e-8bad-ad51f62b2ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ee6c601-650f-4bf5-90dc-3e94a0a06826",
   "metadata": {},
   "source": [
    "**Atenção:** Caso sua máquina não tenha GPU NVIDIA, aparecerá um aviso acima informando que não foram encontrados ***drivers CUDA*** (Compute Unified Device Architeture) que são uma parte essencial do ecossistema de programação paralela da NVIDIA, que permite o uso de GPUs NVIDIA para realizar cálculos intensivos. Porém, já que os drivers não foram encontrados será usada a CPU da máquina ao invés da GPU, o que **não** irá interferir na acurácia do modelo, apenas no tempo de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22469a2-bece-4bb0-8867-b9e84a063ff3",
   "metadata": {},
   "source": [
    "# Coleta e processamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0a258-0d49-4f5c-831f-5b3a06dbe97b",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8898354-b8d7-4202-b6ba-5f5586f79cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_normalized = img_resized / 255.0\n",
    "    return img_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0998c5e-c3dc-4cfa-b782-ae19433c421d",
   "metadata": {},
   "source": [
    "## Balanceamento do DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef594a-b852-42e6-84c1-7d28acf17c5b",
   "metadata": {},
   "source": [
    "Para evitar maior peso em um determinado tipo de dado, foi feito um balanceamento do dataset.\n",
    "\n",
    "Para isso, foi utilizado o método de reamostragem chamado de undersampling, no qual foi selecionado aleatoriamente os dados de uma classe e duplicado até que a quantidade de dados de cada classe se iguale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22091a9-ad92-418e-8c16-893f7077c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset balanceado criado em: ./balanced-dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = './dataset'\n",
    "balanced_dataset_dir = './balanced-dataset'\n",
    "\n",
    "classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "\n",
    "os.makedirs(balanced_dataset_dir, exist_ok=True)\n",
    "\n",
    "image_counts = {}\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(dataset_dir, cls)\n",
    "    image_counts[cls] = len(os.listdir(cls_path))\n",
    "\n",
    "max_images = max(image_counts.values())\n",
    "new_image_counts = {}\n",
    "\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(balanced_dataset_dir, cls), exist_ok=True)\n",
    "    \n",
    "    images = os.listdir(os.path.join(dataset_dir, cls))\n",
    "    \n",
    "    for index, image in enumerate(images):\n",
    "        new_name = f\"{cls}_{index + 1:03d}.jpg\"\n",
    "        shutil.copy(os.path.join(dataset_dir, cls, image), \n",
    "                    os.path.join(balanced_dataset_dir, cls, new_name))\n",
    "    \n",
    "    new_image_counts[cls] = len(images)\n",
    "\n",
    "    current_count = new_image_counts[cls]\n",
    "    while current_count < max_images:\n",
    "        image_to_copy = random.choice(images)\n",
    "        \n",
    "        new_name = f\"{cls}_{current_count + 1:03d}.jpg\"\n",
    "        shutil.copy(os.path.join(dataset_dir, cls, image_to_copy),\n",
    "                    os.path.join(balanced_dataset_dir, cls, new_name))\n",
    "        \n",
    "        current_count += 1\n",
    "\n",
    "print(\"Dataset balanceado criado em:\", balanced_dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1b31e-41cd-4c84-97ec-92d70f180577",
   "metadata": {},
   "source": [
    "# Criando CNN (rede neural convolucional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1aa94-4c96-440e-bd48-d4e10a17b430",
   "metadata": {},
   "source": [
    "Essa função cria um modelo CNN que passa por algumas camadas convolucionais para extrair características das imagens, nas quais foram utilizados as funções:\n",
    "\n",
    "- ReLU\n",
    "    - Função de ativação que permite que a rede neural aprenda padrões complexos\n",
    "- Pooling\n",
    "    - Operação de amostragem usada em redes convolucionais para reduzir as dimensões e/ou tamanho da imagem ou das saídas das camadas convolucionais, porém, preservando suas características mais importantes\n",
    "- Normalização de Batch\n",
    "    - Normalização de dados de entrada para que a rede neural possa aprender mais rapidamente\n",
    "- Softmax\n",
    "    - Função de ativação que converte um vetor de valores reais em uma distribuição de probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce7a26f-8c9f-4669-94e3-4da422619169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3_class_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(x)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_2_class_cnn(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(x)\n",
    "    \n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (224, 224, 1)\n",
    "\n",
    "generalist_cnn_model = create_3_class_cnn(input_shape)\n",
    "lordosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "kiphosis_cnn_model = create_2_class_cnn(input_shape)\n",
    "\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601761ec-a836-41dd-af33-c4642b49cc16",
   "metadata": {},
   "source": [
    "# Gerador de alterações nas imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35f141-9397-4b4d-9d21-981c2c2c9585",
   "metadata": {},
   "source": [
    "Para melhorar a amplitude do treinamento, foi utilizado o ImageDataGenerator, que trás alterações em rotação, deslocamento, zoom, brilho e direção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976a9fae-68a2-4264-a723-96f3d5f0da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    brightness_range = [0.8, 1.2],\n",
    "    validation_split = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219c2be-805d-49a7-ae56-e99642f0d6e4",
   "metadata": {},
   "source": [
    "# Pré-processamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3717d777-9879-44dc-8894-cfa6a02d247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "generalist_classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "\n",
    "generalist_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "generalist_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation'\n",
    ")\n",
    "\n",
    "generalist_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b825ce-5556-4829-8eab-33f63189c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "lordosis_classes = ['healthy', 'lordosis']\n",
    "\n",
    "lordosis_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    classes = lordosis_classes\n",
    ")\n",
    "\n",
    "lordosis_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    classes = lordosis_classes\n",
    ")\n",
    "\n",
    "lordosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d4cc02-6b6e-4d4a-ab61-759ac5f27ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "kiphosis_classes = ['healthy', 'kyphosis']\n",
    "\n",
    "kiphosis_train_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    classes = kiphosis_classes\n",
    ")\n",
    "\n",
    "kiphosis_validation_generator = train_datagen.flow_from_directory(\n",
    "    './balanced-dataset',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = 4,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation',\n",
    "    classes = kiphosis_classes\n",
    ")\n",
    "\n",
    "kiphosis_cnn_model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46560c27-84ac-4454-bd00-cb0372a32f3e",
   "metadata": {},
   "source": [
    "# Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee41bfe6-3b16-4221-9528-ffb26cfb2046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 4s/step - accuracy: 0.3358 - loss: 4.4626 - val_accuracy: 0.3333 - val_loss: 4.3427 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.4501 - loss: 4.2209 - val_accuracy: 0.4000 - val_loss: 3.8421 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - accuracy: 0.3832 - loss: 3.9825 - val_accuracy: 0.3333 - val_loss: 3.6575 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.7386 - loss: 3.3767 - val_accuracy: 0.6000 - val_loss: 3.5524 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3s/step - accuracy: 0.6695 - loss: 3.2548 - val_accuracy: 0.4667 - val_loss: 3.5182 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 3s/step - accuracy: 0.6395 - loss: 3.3289 - val_accuracy: 0.4000 - val_loss: 3.6569 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 4s/step - accuracy: 0.4002 - loss: 3.8582 - val_accuracy: 0.4667 - val_loss: 3.5630 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.5608 - loss: 3.3217 - val_accuracy: 0.4000 - val_loss: 3.4571 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5s/step - accuracy: 0.5125 - loss: 3.4063 - val_accuracy: 0.5333 - val_loss: 3.3820 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - accuracy: 0.6813 - loss: 3.2068 - val_accuracy: 0.5333 - val_loss: 3.3022 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 4s/step - accuracy: 0.6536 - loss: 3.0940 - val_accuracy: 0.5333 - val_loss: 3.3196 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 4s/step - accuracy: 0.5285 - loss: 3.4081 - val_accuracy: 0.4000 - val_loss: 3.4553 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.7043 - loss: 2.9966 - val_accuracy: 0.7333 - val_loss: 3.1115 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.7800 - loss: 2.9218 - val_accuracy: 0.5333 - val_loss: 3.0541 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - accuracy: 0.5349 - loss: 3.1879 - val_accuracy: 0.5333 - val_loss: 3.1630 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.6136 - loss: 3.0128 - val_accuracy: 0.6000 - val_loss: 2.9139 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.5831 - loss: 3.1146 - val_accuracy: 0.7333 - val_loss: 2.8437 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.5690 - loss: 2.9786 - val_accuracy: 0.6000 - val_loss: 2.9340 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3s/step - accuracy: 0.7300 - loss: 2.9445 - val_accuracy: 0.6667 - val_loss: 2.8744 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.6997 - loss: 2.7625 - val_accuracy: 0.6000 - val_loss: 2.7812 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.6536 - loss: 2.8326 - val_accuracy: 0.8000 - val_loss: 2.8294 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.6304 - loss: 3.0699 - val_accuracy: 0.6000 - val_loss: 2.9794 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3s/step - accuracy: 0.5685 - loss: 3.1808 - val_accuracy: 0.5333 - val_loss: 3.1784 - learning_rate: 0.0010\n",
      "Epoch 24/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.7468 - loss: 2.6705 - val_accuracy: 0.6667 - val_loss: 2.9526 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3s/step - accuracy: 0.7045 - loss: 3.1703 - val_accuracy: 0.6000 - val_loss: 2.7951 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.6977 - loss: 2.8456 - val_accuracy: 0.7333 - val_loss: 2.7838 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - accuracy: 0.6798 - loss: 2.9557 - val_accuracy: 0.6667 - val_loss: 2.7424 - learning_rate: 2.5000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.8260 - loss: 2.6115 - val_accuracy: 0.6667 - val_loss: 2.7061 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.7493 - loss: 2.6402 - val_accuracy: 0.8000 - val_loss: 2.6413 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.5690 - loss: 2.8202 - val_accuracy: 0.8000 - val_loss: 2.5627 - learning_rate: 2.5000e-04\n",
      "Epoch 1/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 3s/step - accuracy: 0.4369 - loss: 4.0205 - val_accuracy: 0.6000 - val_loss: 3.6167 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6113 - loss: 3.7004 - val_accuracy: 0.5000 - val_loss: 3.5367 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6202 - loss: 3.4296 - val_accuracy: 0.8000 - val_loss: 3.2938 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6274 - loss: 3.3586 - val_accuracy: 0.5000 - val_loss: 3.3104 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.8821 - loss: 3.0868 - val_accuracy: 0.5000 - val_loss: 3.2261 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.3946 - loss: 3.6550 - val_accuracy: 0.3000 - val_loss: 3.1702 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6982 - loss: 3.1323 - val_accuracy: 0.5000 - val_loss: 3.1688 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6446 - loss: 3.0654 - val_accuracy: 0.7000 - val_loss: 2.9601 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5440 - loss: 3.1050 - val_accuracy: 0.7000 - val_loss: 2.9332 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6036 - loss: 3.3044 - val_accuracy: 0.8000 - val_loss: 2.8968 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9220 - loss: 2.6612 - val_accuracy: 0.8000 - val_loss: 2.8570 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5631 - loss: 3.1752 - val_accuracy: 0.4000 - val_loss: 2.8747 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6095 - loss: 2.9308 - val_accuracy: 0.7000 - val_loss: 2.8223 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.7101 - loss: 3.0247 - val_accuracy: 0.8000 - val_loss: 2.8382 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6583 - loss: 2.6837 - val_accuracy: 0.6000 - val_loss: 3.0425 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.7250 - loss: 2.6356 - val_accuracy: 0.8000 - val_loss: 2.6874 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6637 - loss: 2.9430 - val_accuracy: 0.5000 - val_loss: 2.9824 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.7548 - loss: 2.6842 - val_accuracy: 0.6000 - val_loss: 2.7310 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8905 - loss: 2.4142 - val_accuracy: 0.9000 - val_loss: 2.5245 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5714 - loss: 2.9324 - val_accuracy: 0.7000 - val_loss: 2.7319 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.5732 - loss: 2.8460 - val_accuracy: 0.6000 - val_loss: 2.7747 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6524 - loss: 2.7391 - val_accuracy: 0.6000 - val_loss: 2.7653 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6006 - loss: 2.7772 - val_accuracy: 0.6000 - val_loss: 2.5450 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6095 - loss: 2.6968 - val_accuracy: 0.9000 - val_loss: 2.4185 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6565 - loss: 2.6872 - val_accuracy: 0.9000 - val_loss: 2.4656 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8238 - loss: 2.4116 - val_accuracy: 1.0000 - val_loss: 2.3258 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.8345 - loss: 2.4233 - val_accuracy: 0.9000 - val_loss: 2.4379 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.8125 - loss: 2.4265 - val_accuracy: 0.6000 - val_loss: 2.4958 - learning_rate: 5.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7155 - loss: 2.5633 - val_accuracy: 0.8000 - val_loss: 2.5164 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.8208 - loss: 2.4601 - val_accuracy: 0.7000 - val_loss: 2.5600 - learning_rate: 2.5000e-04\n",
      "Epoch 1/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4s/step - accuracy: 0.6423 - loss: 4.0180 - val_accuracy: 0.5000 - val_loss: 3.6065 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.6929 - loss: 3.5792 - val_accuracy: 0.4000 - val_loss: 3.5666 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6107 - loss: 3.6204 - val_accuracy: 0.9000 - val_loss: 3.1675 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6542 - loss: 3.1910 - val_accuracy: 0.7000 - val_loss: 3.1647 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.5899 - loss: 3.2126 - val_accuracy: 0.5000 - val_loss: 3.2349 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.8018 - loss: 2.8206 - val_accuracy: 0.5000 - val_loss: 3.1421 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.5988 - loss: 2.9927 - val_accuracy: 0.5000 - val_loss: 3.1547 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.5452 - loss: 3.1485 - val_accuracy: 0.5000 - val_loss: 2.9592 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8006 - loss: 2.6722 - val_accuracy: 0.6000 - val_loss: 2.8291 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9054 - loss: 2.5109 - val_accuracy: 0.7000 - val_loss: 2.6172 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8185 - loss: 2.6335 - val_accuracy: 0.8000 - val_loss: 2.6019 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9440 - loss: 2.3844 - val_accuracy: 0.9000 - val_loss: 2.4842 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.8476 - loss: 2.4453 - val_accuracy: 0.9000 - val_loss: 2.4105 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 0.7470 - loss: 2.9301 - val_accuracy: 0.9000 - val_loss: 2.3999 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.7274 - loss: 2.8651 - val_accuracy: 0.8000 - val_loss: 2.4721 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.7798 - loss: 2.4564 - val_accuracy: 0.8000 - val_loss: 2.5054 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 2.8958 - val_accuracy: 0.7000 - val_loss: 2.5100 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9042 - loss: 2.2746 - val_accuracy: 0.9000 - val_loss: 2.3579 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - accuracy: 0.9601 - loss: 2.3203 - val_accuracy: 0.9000 - val_loss: 2.3234 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.6583 - loss: 2.5975 - val_accuracy: 0.8000 - val_loss: 2.4069 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6756 - loss: 2.4220 - val_accuracy: 0.8000 - val_loss: 2.4805 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3s/step - accuracy: 0.8196 - loss: 2.3306 - val_accuracy: 0.8000 - val_loss: 2.3869 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.9810 - loss: 2.1192 - val_accuracy: 0.8000 - val_loss: 2.3629 - learning_rate: 2.5000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.6595 - loss: 2.4725 - val_accuracy: 0.8000 - val_loss: 2.3073 - learning_rate: 2.5000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9411 - loss: 2.1745 - val_accuracy: 0.9000 - val_loss: 2.3134 - learning_rate: 2.5000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9363 - loss: 2.1409 - val_accuracy: 0.9000 - val_loss: 2.2879 - learning_rate: 2.5000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7929 - loss: 2.3605 - val_accuracy: 0.8000 - val_loss: 2.3088 - learning_rate: 2.5000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9411 - loss: 2.0618 - val_accuracy: 0.9000 - val_loss: 2.3128 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7798 - loss: 2.3348 - val_accuracy: 0.9000 - val_loss: 2.2266 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8833 - loss: 2.2025 - val_accuracy: 0.8000 - val_loss: 2.2430 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "generalist_history = generalist_cnn_model.fit(\n",
    "    generalist_train_generator,\n",
    "    validation_data = generalist_validation_generator,\n",
    "    epochs = 30,\n",
    "    callbacks = [lr_scheduler]\n",
    ")\n",
    "\n",
    "lordosis_history = lordosis_cnn_model.fit(\n",
    "    lordosis_train_generator,\n",
    "    validation_data = lordosis_validation_generator,\n",
    "    epochs = 30,\n",
    "    callbacks = [lr_scheduler]\n",
    ")\n",
    "\n",
    "kiphosis_history = kiphosis_cnn_model.fit(\n",
    "    kiphosis_train_generator,\n",
    "    validation_data = kiphosis_validation_generator,\n",
    "    epochs = 30,\n",
    "    callbacks = [lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35044267-0e67-42ae-a3ab-364cdacee14e",
   "metadata": {},
   "source": [
    "### Porque resultados sempre são diferentes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cbe39-de1c-4876-a5ab-750edb21840e",
   "metadata": {},
   "source": [
    "- Ao inicializar o modelo os pesos da rede neural são inicializados de forma aleatória, e como o processo de otimização do modelo começa a partir de diferentes pontos, cada execução pode levar a resultados diferentes, por isso, cada vez que o código for executado os resultados não serão exatamente os mesmos, mas aproximados.\r\n",
    "- \n",
    "Image Augmentation: técnicas de rotação, deslocamento, zoom e etc. aplicadas na imagem para criar novas variações para realizar o treinamento do modelo, sendo também um processo aleatório.\n",
    "- \r\n",
    "Shuffling: Durante o treinamento, os dados de treino são embaralhados a cada época. Isso garante que o modelo não aprenda de forma dependente da ordem dos exemplos, mas também pode fazer com que os resultados variem.io."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef43cf-cc97-4ac7-8f19-fbe6a558cef2",
   "metadata": {},
   "source": [
    "# Avaliação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97b22b7-3e04-4edf-8ea3-3969738cb01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.7583 - loss: 2.6461\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 761ms/step - accuracy: 0.8062 - loss: 2.4203\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 845ms/step - accuracy: 0.9187 - loss: 2.1676\n",
      "Validação modelo generalista - Loss: 2.7754969596862793, Acurácia: 0.6666666865348816\n",
      "Validação modelo de lordose - Loss: 2.4447550773620605, Acurácia: 0.800000011920929\n",
      "Validação modelo de cifose - Loss: 2.192209482192993, Acurácia: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "generalist_val_loss, generalist_val_acc = generalist_cnn_model.evaluate(generalist_validation_generator)\n",
    "lordosis_val_loss, lordosis_val_acc = lordosis_cnn_model.evaluate(lordosis_validation_generator)\n",
    "kiphosis_val_loss, kiphosis_val_acc = kiphosis_cnn_model.evaluate(kiphosis_validation_generator)\n",
    "\n",
    "print(f\"Validação modelo generalista - Loss: {generalist_val_loss}, Acurácia: {generalist_val_acc}\")\n",
    "print(f\"Validação modelo de lordose - Loss: {lordosis_val_loss}, Acurácia: {lordosis_val_acc}\")\n",
    "print(f\"Validação modelo de cifose - Loss: {kiphosis_val_loss}, Acurácia: {kiphosis_val_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1dcd0-85c9-4f6d-a4d0-dc1926a7ba89",
   "metadata": {},
   "source": [
    "# Validação Cruzada dos modelos e ajustes do Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee234d9-007f-4fe0-b862-8f3fe514ec52",
   "metadata": {},
   "source": [
    "#### Contruir o modelo com hiperparâmetros variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "755cfbe6-ef05-4ba3-82dc-f4dda5ba152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, num_classes):\n",
    "    inputs = layers.Input(shape=(224, 224, 1))\n",
    "    \n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(inputs)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    x = base_model(x)\n",
    "    \n",
    "    x = layers.Conv2D(hp.Int('conv_units', min_value=32, max_value=128, step=32), \n",
    "                      (3, 3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hp.Int('dense_units', min_value=64, max_value=256, step=64), \n",
    "                     activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.Dropout(hp.Float('dropout', min_value=0.3, max_value=0.7, step=0.1))(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', ['adam', 'rmsprop']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245e6b2-656a-42fe-bc56-246bc50e1941",
   "metadata": {},
   "source": [
    "#### Parâmetros de Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f48f93-a80e-45ef-9154-5be3b363543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_shape = (224, 224, 1)\n",
    "dataset_path = './balanced-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e6ebb-5be7-4bc5-9d08-1b740212e820",
   "metadata": {},
   "source": [
    "#### Ajuste dos hiperparâmetros e validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99571296-5cf6-4e03-bbcf-5ee861b0239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_and_tuning(classes, num_classes, epochs=10, n_splits=3):\n",
    "    all_data_generator = train_datagen.flow_from_directory(\n",
    "        dataset_path,\n",
    "        target_size=(224, 224),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        classes=classes\n",
    "    )\n",
    "\n",
    "    n_samples = all_data_generator.samples \n",
    "    if n_splits > n_samples:\n",
    "        raise ValueError(f\"Número de splits {n_splits} é maior que o número de amostras {n_samples}.\")\n",
    "\n",
    "    def model_builder(hp):\n",
    "        return build_model(hp, num_classes)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=epochs,\n",
    "        factor=3,\n",
    "        directory='tuning_results',\n",
    "        project_name='hyperparameter_tuning'\n",
    "    )\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "    split = 1\n",
    "    for train_index, val_index in kfold.split(range(n_samples)):\n",
    "        print(f'Fold {split} de {n_splits}')\n",
    "        \n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            dataset_path,\n",
    "            target_size=(224, 224),\n",
    "            color_mode='grayscale',\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training',\n",
    "            classes=classes\n",
    "        )\n",
    "        \n",
    "        validation_generator = train_datagen.flow_from_directory(\n",
    "            dataset_path,\n",
    "            target_size=(224, 224),\n",
    "            color_mode='grayscale',\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            classes=classes\n",
    "        )\n",
    "\n",
    "        tuner.search(train_generator, validation_data=validation_generator, epochs=epochs)\n",
    "\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(f'Melhores hiperparâmetros no Fold {split}:')\n",
    "        print(f'Conv2D units: {best_hps.get(\"conv_units\")}')\n",
    "        print(f'Dense units: {best_hps.get(\"dense_units\")}')\n",
    "        print(f'Dropout: {best_hps.get(\"dropout\")}')\n",
    "        print(f'Otimizador: {best_hps.get(\"optimizer\")}')\n",
    "\n",
    "        split += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a7e32-538d-49b0-9242-9daef7979991",
   "metadata": {},
   "source": [
    "#### Ajustes para os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6e54dd-f6cd-445c-a82a-a922f3133fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 03m 13s]\n",
      "val_accuracy: 0.7333333492279053\n",
      "\n",
      "Best val_accuracy So Far: 0.9333333373069763\n",
      "Total elapsed time: 02h 28m 13s\n",
      "Melhores hiperparâmetros no Fold 1:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Fold 2 de 3\n",
      "Found 36 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n",
      "Melhores hiperparâmetros no Fold 2:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Fold 3 de 3\n",
      "Found 36 images belonging to 3 classes.\n",
      "Found 15 images belonging to 3 classes.\n",
      "Melhores hiperparâmetros no Fold 3:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Found 24 images belonging to 2 classes.\n",
      "Reloading Tuner from tuning_results/hyperparameter_tuning/tuner0.json\n",
      "Fold 1 de 3\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Melhores hiperparâmetros no Fold 1:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Fold 2 de 3\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Melhores hiperparâmetros no Fold 2:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Fold 3 de 3\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Melhores hiperparâmetros no Fold 3:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Found 24 images belonging to 2 classes.\n",
      "Reloading Tuner from tuning_results/hyperparameter_tuning/tuner0.json\n",
      "Fold 1 de 3\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Melhores hiperparâmetros no Fold 1:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Fold 2 de 3\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Melhores hiperparâmetros no Fold 2:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n",
      "Fold 3 de 3\n",
      "Found 24 images belonging to 2 classes.\n",
      "Found 10 images belonging to 2 classes.\n",
      "Melhores hiperparâmetros no Fold 3:\n",
      "Conv2D units: 32\n",
      "Dense units: 64\n",
      "Dropout: 0.3\n",
      "Otimizador: rmsprop\n"
     ]
    }
   ],
   "source": [
    "generalist_classes = ['healthy', 'kyphosis', 'lordosis']\n",
    "cross_val_and_tuning(generalist_classes, num_classes=3)\n",
    "\n",
    "lordosis_classes = ['healthy', 'lordosis']\n",
    "cross_val_and_tuning(lordosis_classes, num_classes=2)\n",
    "\n",
    "kiphosis_classes = ['healthy', 'kyphosis']\n",
    "cross_val_and_tuning(kiphosis_classes, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8ff09-058e-4022-8f28-1f5073ec8379",
   "metadata": {},
   "source": [
    "# Função de predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eebde51d-b5f1-4596-a383-2bdb0985816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "Predição (modelo generalista): [[0.7788245  0.02539187 0.19578362]]\n",
      "Classe prevista (modelo generalista): healthy\n",
      "Predição (modelo de lordose): [[0.85335857 0.14664136]]\n",
      "Classe prevista (modelo de lordose): healthy\n",
      "Predição (modelo de cifose): [[0.70608634 0.2939137 ]]\n",
      "Classe prevista (modelo de cifose): healthy\n"
     ]
    }
   ],
   "source": [
    "def predict_image(image_path, model):\n",
    "    img = preprocess_image(image_path)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    return predicted_class, prediction\n",
    "\n",
    "generalist_predicted_class, generalist_result = predict_image('./test-data/001.jpg', generalist_cnn_model)\n",
    "lordosis_predicted_class, lordosis_result = predict_image('./test-data/001.jpg', lordosis_cnn_model)\n",
    "kiphosis_predicted_class, kiphosis_result = predict_image('./test-data/001.jpg', kiphosis_cnn_model)\n",
    "\n",
    "print(f'Predição (modelo generalista): {generalist_result}')\n",
    "print(f'Classe prevista (modelo generalista): {generalist_classes[generalist_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de lordose): {lordosis_result}')\n",
    "print(f'Classe prevista (modelo de lordose): {lordosis_classes[lordosis_predicted_class]}')\n",
    "\n",
    "print(f'Predição (modelo de cifose): {kiphosis_result}')\n",
    "print(f'Classe prevista (modelo de cifose): {kiphosis_classes[kiphosis_predicted_class]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
